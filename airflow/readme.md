# Airflow DAGs para Pipelines FIFA ML

Este directorio contiene los **Apache Airflow DAGs** utilizados para orquestar los pipelines de **Kedro** del proyecto *FIFA ML*.  
La estructura sigue el est√°ndar ense√±ado en clases y replica buenas pr√°cticas de MLOps del repositorio de referencia.

---

## Objetivo

Transformar los pipelines del proyecto Kedro en DAGs operables desde Airflow, permitiendo:

- Automatizar la ingesta, preparaci√≥n y procesamiento de datos.  
- Entrenar modelos de forma peri√≥dica.  
- Ejecutar el flujo completo del proyecto mediante un **DAG maestro**.  
- Realizar ejecuciones manuales o experimentales de forma controlada.

---

## Propoc√≠to de los archivos 

### 1. `FIFA_ml_pipeline`
- **Tipo:** DAG maestro  
- **Prop√≥sito:** Ejecuta el flujo completo (ETL ‚Üí ML ‚Üí Reporting).  
- **Uso:** Producci√≥n / ejecuci√≥n diaria.

### Ejemplo (modificar)
start ‚Üí data_processing ‚Üí data_science ‚Üí reporting ‚Üí end

---

## DAGs por frecuencia (pr√≥ximos a implementar)

Estos DAGs dividen el proyecto en procesos independientes:

### 2. `FIFA_daily_data_processing`
- Corre cada pocas horas.  
- Ejecuta √∫nicamente el pipeline de preparaci√≥n de datos.

### 3. `FIFA_weekly_model_training`
- Corre semanalmente.  
- Ejecuta el pipeline de entrenamiento del modelo.

### 4. `FIFA_on_demand`
- Sin programaci√≥n autom√°tica.  
- Sirve para pruebas, debugging o ejecuciones manuales.

---

## KedroOperator

Todos los DAGs utilizan un **operador personalizado** que permite ejecutar nodos o pipelines de Kedro dentro de tareas Airflow.

Este operador manejar√°:

- Logs  
- Errores  
- Par√°metros de ejecuci√≥n  
- Rutas del proyecto  
- Integraci√≥n con configuraciones de Airflow  

---

## Estructura

```txt
dags/
‚îú‚îÄ‚îÄ operators/
‚îÇ   ‚îî‚îÄ‚îÄ kedro_operator.py         # Operador personalizado para integrar Kedro
‚îú‚îÄ‚îÄ config.py                     # Configuraci√≥n com√∫n (rutas, package_name, etc.)
‚îú‚îÄ‚îÄ FIFA_ml_pipeline.py           # DAG maestro (ETL + ML + Reporting)
‚îú‚îÄ‚îÄ FIFA_daily_data_processing.py # Procesamiento de datos (frecuencia alta)
‚îú‚îÄ‚îÄ FIFA_weekly_model_training.py # Entrenamiento de modelo semanal
‚îî‚îÄ‚îÄ FIFA_on_demand.py             # Ejecuci√≥n manual / experimental
```

## Ejecuci√≥n 

## Referencia 

Repositorio: [Link](https://github.com/Giocrisrai/kedro_tutorial_test/blob/main/dags/README.md)


# Pasos Previos para Ejecutar Airflow con Docker Compose

Antes de correr Airflow y probar los DAGs, sigue estos pasos m√≠nimos.

## 1. Estar en la carpeta correcta
Debes ejecutar los comandos desde tu m√°quina (host), **no dentro de un contenedor**.

Ub√≠cate en la carpeta donde est√° tu archivo:
compose/docker-compose.airflow.yml

Ejemplo:

```bash
cd compose
```

2. Construir o reconstruir los servicios de Airflow

Ejecuta esto si agregas nuevos DAGs, cambios en dependencias o archivos importantes:

docker-compose -f docker-compose.airflow.yml build
docker-compose -f compose/docker-compose.airflow.yml up -d

3. Levantar Airflow

Este comando inicia Postgres, Scheduler y Webserver dentro de contenedores:

docker-compose -f docker-compose.airflow.yml up -d #dentro 
docker-compose -f compose/docker-compose.airflow.yml up -d # fuera de raiz

4. Verificar que Airflow est√° corriendo

Abrir en el navegador:

http://localhost:8080


Usuario y contrase√±a por defecto (si no se cambi√≥):

user: airflow
pass: airflow

5. Ver logs si algo falla

Scheduler:

docker logs airflow-scheduler -f


Webserver:

docker logs airflow-webserver -f

6. Actualizar DAGs sin reiniciar todo

Si solo modificaste un DAG o un archivo dentro de dags/, puedes reiniciar solo los servicios clave:

docker-compose -f docker-compose.airflow.yml restart airflow-scheduler
docker-compose -f docker-compose.airflow.yml restart airflow-webserver

7. Apagar Airflow cuando termines
docker-compose -f docker-compose.airflow.yml down


# ultimo visto (ver si es necesario anotarlo)

Inicializar Airflow solo la primera vez
docker-compose -f compose/docker-compose.airflow.yml up airflow-init

3. Levantar webserver + scheduler
docker-compose -f compose/docker-compose.airflow.yml up -d

4. Ver logs (si algo falla)
docker-compose -f compose/docker-compose.airflow.yml logs -f webserver

5. Detener todo
docker-compose -f compose/docker-compose.airflow.yml down


## No veo los dags?

puedes verificar en  docker-compose.yaml y revisa que tenga:

volumes:
  - ./dags:/opt/airflow/dags
  - ./logs:/opt/airflow/logs
  - ./plugins:/opt/airflow/plugins

---

## Errores solucionados 

**No se detecta los DAGs**

- Cambio en las rutas de volumes en docker-compose.airflow.yml: - ../airflow/dags:/opt/airflow/dags 

Esto depende de tu proyecto, en mi caso, no se estaba leyendo la carpeta que tenia los dags ya que tenia otra ruta configurada. 
Luego de eso, de pudo detectar los DAGs.

**No detecta los otros archivos .py**

- Solucion: 

Descripcion: 

---


Dockerfile de Airflow ‚Äî Explicaci√≥n R√°pida

Este proyecto utiliza Airflow + Kedro, por lo que es necesario usar un Dockerfile personalizado para Airflow.
La raz√≥n principal es que Airflow no se ejecuta en tu computador, sino dentro de contenedores Docker, y estos contenedores no tienen Kedro instalado por defecto.

üîç ¬øPor qu√© usar un Dockerfile para Airflow?

Si no se usa un Dockerfile personalizado, dentro del contenedor aparecer√°n errores como:

ModuleNotFoundError: No module named 'kedro'


Esto ocurre porque el contenedor de Airflow no tiene:

Kedro instalado

El operador personalizado kedro_operator

Acceso interno al proyecto Kedro

üß© ¬øQu√© soluciona el Dockerfile?

El dockerfile-airflow permite:

‚úîÔ∏è Instalar Kedro dentro del contenedor Airflow

‚úîÔ∏è Instalar el operador kedro_operator

‚úîÔ∏è Copiar o montar las rutas necesarias de Airflow (dags, plugins, etc.)

‚úîÔ∏è Permitir que los DAGs interact√∫en con el proyecto Kedro