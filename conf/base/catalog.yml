# Here you can define all your datasets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://docs.kedro.org/en/stable/data/data_catalog.html


# Aqui podemos definir estructuras donde
# nosotros podemos definir nuestros datasets
# usando una sintaxis simple de YAML.
# Con esto podemos llamar los datasets
# desde cualquier parte del proyecto.

DataSetFIFA22:
  type: pandas.CSVDataset
  filepath: data/01_raw/FIFA22_official_data.csv
  load_args:
    sep: ","
    decimal: "."
  save_args:
    index: False  
 
DataSetFIFA21:
  type: pandas.CSVDataset
  filepath: data/01_raw/FIFA21_official_data.csv
  load_args:
    sep: ","
    decimal: "."
  save_args:
    index: False

DataSetFIFA20:
  type: pandas.CSVDataset
  filepath: data/01_raw/FIFA20_official_data.csv
  load_args:
    sep: ","
    decimal: "."
  save_args:
    index: False

# Pasos siguientes:
#Aqui estan el dataset ya preprocesado
# de FIFA 21, se debe crear los otros dos
preprocess_fifa_22:
  type: pandas.ParquetDataset # Ahora definimos que la salida sea tipo parquet para que sea mas liviano 
  filepath: data/02_intermediate/preprocess_fifa_22.parquet     

preprocess_fifa_21:
  type: pandas.ParquetDataset # Ahora definimos que la salida sea tipo parquet para que sea mas liviano 
  filepath: data/02_intermediate/preprocess_fifa_21.parquet     

preprocess_fifa_20:
  type: pandas.ParquetDataset # Ahora definimos que la salida sea tipo parquet para que sea mas liviano 
  filepath: data/02_intermediate/preprocess_fifa_20.parquet 



FIFA22_processed_con_transformacion2_columns:
  type: pandas.ParquetDataset 
  filepath: data/02_intermediate/FIFA22_processed_con_transformacion2_columns.parquet  

FIFA21_processed_con_transformacion2_columnss:
  type: pandas.ParquetDataset
  filepath: data/02_intermediate/FIFA21_processed_con_transformacion2_columns.parquet

FIFA20_processed_con_transformacion2_columns:
  type: pandas.ParquetDataset 
  filepath: data/02_intermediate/FIFA20_processed_con_transformacion2_columns.parquet  

# OJO, al pasar la evaluacion 3 con las tecnicas de modelo no supervizados, crearemos un nuevo dataset limpio, asi que ahi veremos si se elimina o se mantiene
# ¿Es necesario tenerlo ahora? [] <-- si es un no, entonces no se elimina para tomarlo en cuenta en futuros proyecto.
model_input_table:
  type: pandas.ParquetDataset 
  filepath: data/05_model_input/model_input_table.parquet #lo definimos en primary porque ya tendremos nuestros datasets unidos

  # Para report utilizar la libreria plotlib al utilizar Kedro viz

#Etapa modeloado 

#Ahora que en el primero nodo del pipeline de modelado regression_models
#No tiene una salida de tipo pandas porque es un diccionario
#Entonces definimos un nuevo dataset de tipo PickleDataset


#GridSearchCV modelo
# versioned: true para versionar los modelos o guardarlos con versiones

# GridSearchCV modelos

# Salidas de los modelos de regresion 

grid_linear_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_linear_model.pkl
  versioned: true

grid_linear_multiple_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_linear_multiple_model.pkl
  versioned: true

grid_svr_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_svr_model.pkl
  versioned: true

grid_decision_tree_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_decision_tree_model.pkl
  versioned: true

grid_randomforest_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_randomforest_model.pkl
  versioned: true


#Salidas de los modelos entrenados de clasificación

grid_logistic_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_logistic_model_classification.pkl
  versioned: true

grid_knn_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_knn_model_classification.pkl
  versioned: true

grind_svc_cv_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grind_svc_cv_model_classification.pkl
  versioned: true

grid_decision_tree_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_decision_tree_model_classification.pkl  
  versioned: true

grid_random_forest_model_classification:  
  type: pickle.PickleDataset 
  filepath: data/06_models/grid_random_forest_model_classification.pkl
  versioned: true

# Entradas de para los modelos, train y test.

#TRAIN TEST REGRESION

X_train_regression:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_train_regression.csv
  save_args:
    index: False

X_test_regression:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_test_regression.csv
  save_args:
    index: False

y_test_regression:
  type: pandas.CSVDataset
  filepath: data/05_model_input/y_test_regression.csv
  save_args:
    index: False

# TRAIN TEST CLASIFICACION

# -- Features de entrenamiento y prueba --

X_train_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_train_class.csv
  save_args:
    index: False

X_test_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_test_class.csv
  save_args:
    index: False

# -- Target de entrenamiento y prueba --

y_train_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/y_train_class.csv
  save_args:
    index: False

y_test_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/y_test_class.csv
  save_args:
    index: False

#Reportes de regresion

regression_report_linear_simple:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_linear_simple.csv
  versioned: true

regression_report_linear_multiple:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_linear_multiple.csv
  versioned: true

regression_report_svr:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_svr.csv
  versioned: true

regression_report_decision_tree:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_decision_tree.csv
  versioned: true

regression_report_randomforest:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_randomforest.csv
  versioned: true

# Reportes clasificación

report_logistic_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_logistic_model_classification.csv
  versioned: true

report_knn_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_knn_model_classification.csv
  versioned: true

report_grind_svc_cv_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_grind_svc_cv_model_classification.csv
  versioned: true

report_decision_tree_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_decision_tree_model_classification.csv
  versioned: true

report_random_forest_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_random_forest_model_classification.csv
  versioned: true

# Entradas o salidas de las tecnicas no supervizadas

# aplicaremos tecnicas no supervizadas para limpiar, reducir dimensiones y agregar nuevas columnas con clustering. 

pca_output:
  type: pandas.ParquetDataSet
  filepath: data/04_feature/pca_output.parquet
  description: "Resultado del proceso PCA: datos transformados a componentes principales para reducción de dimensionalidad."

clustered_data:
  type: pandas.ParquetDataSet
  filepath: data/04_feature/clustered_data.parquet
  description: "Datos con etiquetas de clúster asignadas por el modelo de clustering."

datos_limpios_sin_anomalias:
  type: pandas.ParquetDataSet
  filepath: data/04_feature/clean_data_no_anomalies.parquet
  description: "Dataset limpio sin anomalías detectadas, listo para la etapa de clustering o PCA."

clean_dataset:
  type: pandas.ParquetDataSet
  filepath: data/04_feature/clean_dataset.parquet
  description: "Dataset procesado tras limpieza general: valores nulos gestionados, tipos corregidos y datos normalizados."

# -- ojo --

#Nodo final combine_unsupervised_outputs que genera clean_dataset.

#Este clean_dataset reemplaza al model_input_table previo, porque ya incorpora las mejoras del no supervisado: clusters, anomalías filtradas, reducción de variables, etc.

# Luego de hacer el pipeline padre,configurar el pipeline registry, los submodulos pipelines y sus parameters
# ahora hay que hacer que las tecnicas de aprendizaje no supervizado afecten nuestro dataset.

#1. El clean_dataset generado por unsupervised_learning será la base para crear los datasets de:

# X_train_regression, X_test_regression, y_train_regression

# X_train_class, X_test_class, y_train_class, y_test_class 
