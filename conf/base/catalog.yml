# Documentation for this file format can be found in "The Data Catalog"
# Link: https://docs.kedro.org/en/stable/data/data_catalog.html

# Aqui podemos definir estructuras donde
# nosotros podemos definir nuestros datasets
# usando una sintaxis simple de YAML.
# Con esto podemos llamar los datasets
# desde cualquier parte del proyecto.

DataSetFIFA22:
  type: pandas.CSVDataset
  filepath: data/01_raw/FIFA22_official_data.csv
  load_args:
    sep: ","
    decimal: "."
  save_args:
    index: False  
 
DataSetFIFA21:
  type: pandas.CSVDataset
  filepath: data/01_raw/FIFA21_official_data.csv
  load_args:
    sep: ","
    decimal: "."
  save_args:
    index: False

DataSetFIFA20:
  type: pandas.CSVDataset
  filepath: data/01_raw/FIFA20_official_data.csv
  load_args:
    sep: ","
    decimal: "."
  save_args:
    index: False

# ============================
# Etapa de procesamiento de datos
# ============================

preprocess_fifa_22:
  type: pandas.ParquetDataset # Ahora definimos que la salida sea tipo parquet para que sea mas liviano 
  filepath: data/02_intermediate/preprocess_fifa_22.parquet     

preprocess_fifa_21:
  type: pandas.ParquetDataset # Ahora definimos que la salida sea tipo parquet para que sea mas liviano 
  filepath: data/02_intermediate/preprocess_fifa_21.parquet     

preprocess_fifa_20:
  type: pandas.ParquetDataset # Ahora definimos que la salida sea tipo parquet para que sea mas liviano 
  filepath: data/02_intermediate/preprocess_fifa_20.parquet 


FIFA22_processed_con_transformacion2_columns:
  type: pandas.ParquetDataset 
  filepath: data/02_intermediate/FIFA22_processed_con_transformacion2_columns.parquet  

FIFA21_processed_con_transformacion2_columnss:
  type: pandas.ParquetDataset
  filepath: data/02_intermediate/FIFA21_processed_con_transformacion2_columns.parquet

FIFA20_processed_con_transformacion2_columns:
  type: pandas.ParquetDataset 
  filepath: data/02_intermediate/FIFA20_processed_con_transformacion2_columns.parquet  

# ============================
# Union de datasets y creacion de dataset de entrada para los modelos (Model input table)
# ============================

# OJO, al pasar la evaluacion 3 con las tecnicas de modelo no supervizados, crearemos un nuevo dataset limpio, asi que ahi veremos si se elimina o se mantiene
# ¿Es necesario tenerlo ahora? [] <-- si es un no, entonces no se elimina para tomarlo en cuenta en futuros proyecto.

model_input_table:
  type: pandas.ParquetDataset 
  filepath: data/05_model_input/model_input_table.parquet #lo definimos en primary porque ya tendremos nuestros datasets unidos


# ============================
# Entradas de para los modelos, train y test.
# ============================

#TRAIN TEST REGRESION

X_train_regression:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_train_regression.csv
  save_args:
    index: False

X_test_regression:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_test_regression.csv
  save_args:
    index: False

y_test_regression:
  type: pandas.CSVDataset
  filepath: data/05_model_input/y_test_regression.csv
  save_args:
    index: False

# TRAIN TEST CLASIFICACION

# -- Features de entrenamiento y prueba --

X_train_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_train_class.csv
  save_args:
    index: False

X_test_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/X_test_class.csv
  save_args:
    index: False

# -- Target de entrenamiento y prueba --

y_train_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/y_train_class.csv
  save_args:
    index: False

y_test_class:
  type: pandas.CSVDataset
  filepath: data/05_model_input/y_test_class.csv
  save_args:
    index: False

# ============================
# Etapa de  modelodo
# ============================

# Ahora que en el primero nodo del pipeline de modelado regression_models
# No tiene una salida de tipo pandas porque es un diccionario
# Entonces definimos un nuevo dataset de tipo PickleDataset. 

# ============================
# GridSearchCV modelo
# ============================

# versioned: true para versionar los modelos o guardarlos con versiones

# Salidas de los modelos de regresion 

grid_linear_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_linear_model.pkl
  versioned: true

grid_linear_multiple_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_linear_multiple_model.pkl
  versioned: true

grid_svr_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_svr_model.pkl
  versioned: true

grid_decision_tree_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_decision_tree_model.pkl
  versioned: true

grid_randomforest_model:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_randomforest_model.pkl
  versioned: true

# Salidas de los modelos entrenados de clasificación

grid_logistic_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_logistic_model_classification.pkl
  versioned: true

grid_knn_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_knn_model_classification.pkl
  versioned: true

grind_svc_cv_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grind_svc_cv_model_classification.pkl
  versioned: true

grid_decision_tree_model_classification:
  type: pickle.PickleDataset
  filepath: data/06_models/grid_decision_tree_model_classification.pkl  
  versioned: true

grid_random_forest_model_classification:  
  type: pickle.PickleDataset 
  filepath: data/06_models/grid_random_forest_model_classification.pkl
  versioned: true

# ============================
# Reportes de regresion
# ============================

# Para report utilizar la libreria plotlib al utilizar Kedro viz

regression_report_linear_simple:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_linear_simple.csv
  versioned: true

regression_report_linear_multiple:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_linear_multiple.csv
  versioned: true

regression_report_svr:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_svr.csv
  versioned: true

regression_report_decision_tree:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_decision_tree.csv
  versioned: true

regression_report_randomforest:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_randomforest.csv
  versioned: true

# ============================
# Reportes clasificación
# ============================

report_logistic_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_logistic_model_classification.csv
  versioned: true

report_knn_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/regression_report_knn_model_classification.csv
  versioned: true

report_grind_svc_cv_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_grind_svc_cv_model_classification.csv
  versioned: true

report_decision_tree_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_decision_tree_model_classification.csv
  versioned: true

report_random_forest_model_classification:
  type: pandas.CSVDataset
  filepath: data/08_reporting/report_random_forest_model_classification.csv
  versioned: true

# ============================
#      UNSUPERVISED
# ============================

# Entradas o salidas de las tecnicas no supervizadas
# aplicaremos tecnicas no supervizadas para limpiar,
# reducir dimensiones y agregar nuevas columnas con clustering. 

#  ============================
# Estas tecnicas no supervizadas se aplicaran en el dataset de entrada
# model_input_table, y generaran 3 nuevos datasets 
# clean_dataset > clustered_dataset y unsupervised_processed_dataset. En la que esta ultima sera la nueva base para los modelos supervisados .
# ============================

# Luego de hacer el pipeline padre unsupervised_learning, configurar el pipeline registry, los submodulos pipelines y sus parameters
# ahora hay que hacer que las tecnicas de aprendizaje no supervizado afecten nuestro dataset.

# Pipeline Submodulo anomaly_detection.

clean_dataset:
  type: pandas.ParquetDataset
  filepath: data/04_feature/clean_dataset.parquet

# Ver las salidas de los clusters y reduccion 

# Pipeline Submodulo clustering.

clustered_dataset:
  type: pandas.ParquetDataset
  filepath: data/04_feature/clustered_dataset.parquet

# Pipeline Submodulo dimensionality_reduction.

pca_output:
  type: pandas.ParquetDataset
  filepath: data/04_feature/pca_output.parquet  

unsupervised_processed_dataset:
  type: pandas.ParquetDataset
  filepath: data/04_feature/unsupervised_processed_dataset.parquet

