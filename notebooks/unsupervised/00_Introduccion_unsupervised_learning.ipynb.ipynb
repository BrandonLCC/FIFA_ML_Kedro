{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee788576-a7ca-44d1-abd8-865b9ee77ae3",
   "metadata": {},
   "source": [
    "# *Introducción al Aprendizaje No Supervisado*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3d998-41c2-415b-88be-3b94db33fafc",
   "metadata": {},
   "source": [
    "#### **Importaciones**a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc08d51-2815-46cd-b07c-3aeab637e60f",
   "metadata": {},
   "source": [
    "**Introduccion teorica** - (Benefició propio)\n",
    "\n",
    "Hasta ahora, tan sólo hemos explorado algoritmos y técnicas de Aprendizaje Automático supervisado para desarrollar modelos en los que los datos tenían etiquetas previamente conocidas. \n",
    "\n",
    "En otras palabras, nuestros datos tenían algunas variables objetivo con valores específicos que utilizamos para entrenar nuestros modelos.\n",
    "\n",
    "Sin embargo, cuando se trata de problemas del mundo real, la mayoría de las veces, los datos no vienen con etiquetas predefinidas: tenemos las características de entrada X, pero no tenemos las etiquetas Y. Estos son los modelos de Aprendizaje Automático No Supervisado.\n",
    "\n",
    "El Aprendizaje no Supervisado pretende descubrir patrones previamente desconocidos en los datos, pero la mayoría de las veces estos patrones son aproximaciones deficientes de lo que el Aprendizaje Supervisado puede lograr. \n",
    "\n",
    "Además, dado que no sabe cuáles deberían ser los resultados, no hay forma de determinar cuán precisos son, lo que hace que el Aprendizaje Supervisado sea más aplicable a los problemas del mundo real.\n",
    "\n",
    "El mejor momento para utilizar el Aprendizaje no Supervisado es cuando no se dispone de datos sobre los resultados deseados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434af50c-1844-481c-9e2b-b20c5b1e8fbb",
   "metadata": {},
   "source": [
    "- **Los algoritmos de Aprendizaje no Supervisado manejan datos sin entrenamiento previo**\n",
    "\n",
    "- **Los algoritmos no supervisados funcionan con datos no etiquetados. Su propósito es la exploración**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789819b-efe5-4f1d-8caf-f233c23bc59c",
   "metadata": {},
   "source": [
    "**Algunos ejemplos de  aplicación de este tipo de modelos**\n",
    "    \n",
    "- La **agrupación** en clústeres divide automáticamente el conjunto de datos en grupos en función de sus similitudes.\n",
    "  \n",
    "- La **detección de anomalías** puede descubrir puntos de datos inusuales en su conjunto de datos. Es útil para encontrar transacciones fraudulentas.\n",
    "  \n",
    "- La **minería de asociaciones** Se refiere a descubrir patrones y relaciones comunes entre los distintos atributos de un conjunto de datos. \n",
    "  \n",
    "- Los **modelos de variables latente**s se utilizan ampliamente para el preprocesamiento de datos. Cómo reducir el número de características en un conjunto de datos o descomponer el conjunto de datos en múltiples componentes.\n",
    "\n",
    "**Tecnicas de Unsupervised Learning en Machine learning**\n",
    "```\n",
    " Unsupervised Learning \n",
    "        |\n",
    "data without label (datos sin etiqueta)\n",
    "        |\n",
    "        - -> Clustering\n",
    "        |\n",
    "        | 1. K-means\n",
    "        | 2. K-median\n",
    "        | 3. Hierarchical clustering\n",
    "        | 4. Expectation Maximization\n",
    "        |\n",
    "        - -> Association Analysis\n",
    "        |\n",
    "        | 1. APRIORI \n",
    "        | 2. Eclat\n",
    "        | 3. FP-Growth\n",
    "        |\n",
    "        - -> Dimensionality Reduction\n",
    "\n",
    "          1. Feature Extraction\n",
    "             - Principal Component Analysis\n",
    "          2. Feature Selection\n",
    "             - Wrapper, filter y Embedded Method\n",
    "\n",
    "```\n",
    "\n",
    "**CLUSTERING**\n",
    "\n",
    "En términos básicos, el objetivo de la agrupación es encontrar diferentes grupos dentro de los elementos de los datos. \n",
    "\n",
    "**ANÁLISIS DE ASOCIACIÓN**\n",
    "\n",
    "Se refiere a descubrir patrones y relaciones comunes entre los distintos atributos de un conjunto de datos. \n",
    "\n",
    "La atención se centra en qué atributos o características del conjunto de datos aparecen juntos con frecuencia y cuáles no. \n",
    "\n",
    "**REDUCCIÓN DE LA DIMENSIONALIDAD**\n",
    "\n",
    "Se refiere a reducir características de los datos conservando la información más importante. Es decir, transformar el conjunto de datos a uno de menor dimensión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e98a8-4118-4e90-a397-db0611eecfef",
   "metadata": {},
   "source": [
    "Antes de realizar los algoritmos, necesitamos tener una interpretación clara definiendo los objetivos.\n",
    "\n",
    "**Objetivos**\n",
    "\n",
    "¿Qué queremos obtener del dataset?\n",
    "\n",
    "Agrupar clientes/jugadores/observaciones con características similares.\n",
    "\n",
    "Detectar anomalías (valores fuera de lo normal).\n",
    "\n",
    "Reducir dimensionalidad para visualizar o mejorar modelos posteriores.\n",
    "\n",
    "Identificar patrones escondidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b1095-e05d-4bc5-a0a3-6525c0180137",
   "metadata": {},
   "source": [
    "## **Unsupervised Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc5f66-8612-43a0-961b-058d50f40200",
   "metadata": {},
   "source": [
    "**Analizar los datos (EDA) sin etiquetas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb90a11-396e-4ed4-afc0-1dd808c4f47f",
   "metadata": {},
   "source": [
    "### **1. Clustering**\n",
    "Implementación y comparación de 3 algoritmos (seleccionar al menos 3)\n",
    "\n",
    "- K-Means: Clustering particional basado en centroides  (este)\n",
    "- DBSCAN: Clustering basado en densidad (este)\n",
    "- Hierarchical Clustering: Clustering jerárquico (este)\n",
    "- Gaussian Mixture Models: Clustering probabilístico (penultimo)\n",
    "- OPTICS: Alternativa a DBSCAN para densidad variable (ultimo)\n",
    "  \n",
    "**Métricas utilizadas:** Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index,  Elbow Method, Dendrogramas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3032a4-1793-408e-a2b3-aa46731b369b",
   "metadata": {},
   "source": [
    "### **2. Reducción de dimensionalidad**\n",
    "\n",
    "Implementar al menos 2 técnicas: \n",
    "\n",
    "- PCA: Análisis de componentes principales (varianza explicada, loadings, biplot)\n",
    "- t-SNE: Visualización 2D/3D de alta dimensión \n",
    "- UMAP: Alternativa moderna a t-SNE \n",
    "- SVD/Truncated SVD: Para datos sparse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2806b-cb40-404f-bdf0-b13efe158e3f",
   "metadata": {},
   "source": [
    "### **3. Detección de Anomalías (OPCIONAL)**\n",
    "\n",
    "- Isolation Forest \n",
    "- Local Outlier Factor (LOF) \n",
    "- One-Class SVM \n",
    "- Autoencoders\n",
    "\n",
    "### **4. Reglas de Asociación (OPCIONAL)**\n",
    "- Apriori Algorithm \n",
    "- FP-Growth \n",
    "\n",
    "Métricas: support, confidence, lift \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
